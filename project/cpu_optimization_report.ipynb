{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.cm as cm\n",
    "import os \n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cpu optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we assess different methods to optimize N-body simulations with Python on CPU.\n",
    "This task is approached in two ways: \n",
    "1.  Optimizing the computation of accelerations, for a given single simulation;\n",
    "\n",
    "2. Optimizing independent simulations run in parallel.\n",
    "\n",
    "For the first approach we use the built-in `multiprocessing` and the external `numba` library, while for the second only the former is considered. \n",
    "\n",
    "We find a significant speed-up of the code in both cases, even though optimization fails when we try to mix the two methods, i.e. when running multiple independent simulations, while computing the accelerations in an optimized fashion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using all cores in a single simulation\n",
    "Modern CPUs come with multiple cores that may run multiple computations, if well programmed. A naive approach to estimating the accelerations in a N-body simulation consists in writing two nested for loops, that for each particle compute the force with respect to all others. We refer to this as a *direct* estimation of the forces. Since Python is an interpreted language, direct estimation performs very badly, and indeed we see that only one core is used when using this method (this is done by visually insepcting the cores used, running `htop` in a bash terminal). \n",
    "A first optimization is implemented by exploiting Numpy's broadcasting. We refer to this as a *vectorized* estimation. By doing so, we see that Numpy automatically uses 4 cores, instead of just one (Numpy is written in compiled C). This results in a much faster estimation, as discussed in the assignments of this class.  \n",
    "\n",
    "In a 8-core CPU this is not the best result achievable, since there are 4 unused cores still. Here we explore the idea that, since Numpy uses 4 cores, we should be able to run our estimations at least two times faster, when using a 8-core CPU. The gain obtained will be greater as the number of cores available gets bigger.\n",
    "\n",
    "This is implemented by using both the `multiprocessing` and `numba` libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing\n",
    "\n",
    "Another possibility is to use all the cores available to run multiple independent simulations, allocating a subset of the CPU each. This goes under the name of **multiprocessing**, and each independent simulation is called a **process**.  It is important to remark that each process has an independent memory, thus it does not communicate with the other processes, even though they run on the same CPU at the same time. This feature discriminate it from **multithreading**, which shares a common memory between different threads running in parallel.\n",
    "\n",
    "Multiprocessing is also used to parallelize computations of the acceleration in a single run, by assigning a particles subset to each process.\n",
    "\n",
    "Each process is taken care of by a **worker**. The set of all workers is called **Pool**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <figure>\n",
    "        <img src=\"./attachments_report_cpu_optimization/plots/multiprocessing_vs_multithreading.png\" alt=\"Comparison Image\">\n",
    "    </figure>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba's JIT compilation and parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, Python is an interpreted language meaning that a program directly executes instructions, without requiring them previously to have been compiled into a machine language program [[1]](https://en.wikipedia.org/wiki/Interpreter_(computing)). This causes native Python to be much slower than C++, for example. \n",
    "\n",
    "Fortunately, it is possible to speed up the code using `numba`, which compiles it to machine code “just-in-time” for execution, making it run at native machine code speed, when possible [[2]](https://numba.readthedocs.io/en/stable/user/5minguide.html). Numba works well with Numpy arrays and it is easy to implement in a standard Python script, since it requires to add just a decorator on our functions.\n",
    "\n",
    "Numba offers the possibilty to run the code in a multi-threaded fashion, by setting `parallel=True` inside the decorator. Unfortunately, not all codes can be parallelized in this way. In fact, we have been able to use this feature only on the direct estimation, and not on the vectorized one.\n",
    "\n",
    "We use numba to speed up the estimation of accelerations for a single simulation. We refer to this as *NJIT* estimation, which stands for \"no python - just in time\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and results\n",
    "\n",
    "All the results shown in the plots are obtained on separate .py files.  \n",
    "Let's start by considering parallel accelerations computations on a single evolution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba\n",
    "\n",
    "Here we show the direct and the vectorized functions used, with their numba's counterparts. Notice that numba requires just to add a decorator `@njit` at the beginning of the function, and to use `prange` instead of `range`.\n",
    "\n",
    "In addition, numpy may need some changes in order for numba to understand exactly how some objects look like when compiled. This fact is a clear example of the work of the Python's interpreter that happens \"under the hood\". For example \n",
    "\n",
    "```python\n",
    "acc  = np.zeros([N,3]) # This does not work with numba\n",
    "acc  = np.zeros_like(pos) # Use this instead\n",
    "```\n",
    "Or when using reshape, it is necessary to make a copy of the array, in order to make it C-contiguous in the memory.\n",
    "\n",
    "```python\n",
    "dx = pos[:, 0].reshape(N_particles, 1) - pos[:, 0] # This does not work with numba\n",
    "dx = pos[:, 0].copy().reshape(N_particles, 1) - pos[:, 0]  # Use this instead\n",
    "   \n",
    "```\n",
    "\n",
    "\n",
    "The biggest problems arise when trying to compute a matrix multiplication using `parallel=True`. Indeed, `np.matmul` is not implemented and even after trying to work around this, we have not been able to use our vectorized function in a parallel way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fireworks.ic import ic_random_uniform as ic_random_uniform\n",
    "import numpy as np\n",
    "from numba import prange, njit\n",
    "\n",
    "def acceleration_direct_slow(pos,mass,N,softening):\n",
    "    jerk = None\n",
    "    pot = None\n",
    "\n",
    "    # acc[i,:] ax,ay,az of particle i \n",
    "    acc  = np.zeros([N,3])\n",
    "\n",
    "    for i in range(N-1):\n",
    "        for j in range(i+1,N):\n",
    "            # Compute relative acceleration given\n",
    "            # position of particle i and j\n",
    "            mass_1 = mass[i]\n",
    "            mass_2 = mass[j]\n",
    "\n",
    "            # Compute acceleration of particle i due to particle j\n",
    "            position_1=pos[i,:]\n",
    "            position_2=pos[j,:]\n",
    "            \n",
    "            # Cartesian component of the i,j particles distance\n",
    "            dx = position_1[0] - position_2[0]\n",
    "            dy = position_1[1] - position_2[1]\n",
    "            dz = position_1[2] - position_2[2]\n",
    "            \n",
    "\n",
    "            # Distance module\n",
    "            r = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "\n",
    "            # Cartesian component of the i,j force\n",
    "            acceleration = np.zeros(3)\n",
    "            acceleration[0] = -mass_2 * (5*softening**2 + 2*r**2) * dx / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[1] = -mass_2 * (5*softening**2 + 2*r**2) * dy / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[2] = -mass_2 * (5*softening**2 + 2*r**2) * dz / (2*(r**2 + softening**2)**(5/2))\n",
    "\n",
    "            # Update array with accelerations\n",
    "            acc[i,:] += acceleration\n",
    "            acc[j,:] -= mass_1 * acceleration / mass_2 # because acc_2nbody already multiply by m[j]\n",
    "        \n",
    "    return (acc,jerk,pot)\n",
    "\n",
    "\n",
    "@njit\n",
    "def acceleration_direct_fast(pos,mass,N,softening):\n",
    "    jerk = None\n",
    "    pot = None\n",
    "\n",
    "    # acc[i,:] ax,ay,az of particle i \n",
    "    #acc  = np.zeros([N,3])\n",
    "    acc = np.zeros_like(pos)\n",
    "\n",
    "    for i in range(N-1):\n",
    "        for j in range(i+1,N):\n",
    "            # Compute relative acceleration given\n",
    "            # position of particle i and j\n",
    "            mass_1 = mass[i]\n",
    "            mass_2 = mass[j]\n",
    "\n",
    "            # Compute acceleration of particle i due to particle j\n",
    "            position_1=pos[i,:]\n",
    "            position_2=pos[j,:]\n",
    "            \n",
    "            # Cartesian component of the i,j particles distance\n",
    "            dx = position_1[0] - position_2[0]\n",
    "            dy = position_1[1] - position_2[1]\n",
    "            dz = position_1[2] - position_2[2]\n",
    "            \n",
    "\n",
    "            # Distance module\n",
    "            r = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "\n",
    "            # Cartesian component of the i,j force\n",
    "            acceleration = np.zeros(3)\n",
    "            acceleration[0] = -mass_2 * (5*softening**2 + 2*r**2) * dx / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[1] = -mass_2 * (5*softening**2 + 2*r**2) * dy / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[2] = -mass_2 * (5*softening**2 + 2*r**2) * dz / (2*(r**2 + softening**2)**(5/2))\n",
    "\n",
    "            # Update array with accelerations\n",
    "            acc[i,:] += acceleration\n",
    "            acc[j,:] -= mass_1 * acceleration / mass_2 # because acc_2nbody already multiply by m[j]\n",
    "        \n",
    "    return (acc,jerk,pot)\n",
    "\n",
    "def slow_acceleration_direct_vectorized(pos,N_particles,mass,softening):\n",
    "    dx = pos[:, 0].reshape(N_particles, 1) - pos[:, 0] #broadcasting of (N,) on (N,1) array, obtain distance along x in an (N,N) matrix\n",
    "    dy = pos[:, 1].reshape(N_particles, 1) - pos[:, 1] \n",
    "    dz = pos[:, 2].reshape(N_particles, 1) - pos[:, 2] \n",
    "    \n",
    "    r = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "    r[r==0]=1\n",
    "    \n",
    "    dpos = np.concatenate((dx, dy, dz)).reshape((3,N_particles,N_particles)) \n",
    "    acc = - (dpos* (5*softening**2 + 2*r**2)/(2*(r**2 + softening**2)**(5/2)) @ mass).T\n",
    "    \n",
    "    jerk= None \n",
    "    pot = None\n",
    "\n",
    "    return acc, jerk, pot\n",
    "\n",
    "\n",
    "@njit\n",
    "def fast_acceleration_direct_vectorized(pos,N_particles,mass,softening):\n",
    "   \n",
    "    dx = pos[:, 0].copy().reshape(N_particles, 1) - pos[:, 0] #broadcasting of (N,) on (N,1) array, obtain distance along x in an (N,N) matrix\n",
    "    dy = pos[:, 1].copy().reshape(N_particles, 1) - pos[:, 1] \n",
    "    dz = pos[:, 2].copy().reshape(N_particles, 1) - pos[:, 2] \n",
    "      \n",
    "    r = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "    #r[r==0]=1 not supported on numba\n",
    "    r += np.eye(r.shape[0])\n",
    "    \n",
    "    dpos = np.concatenate((dx, dy, dz)).copy().reshape((3,N_particles,N_particles)) \n",
    "    acc = - np.sum(dpos* (5*softening**2 + 2*r**2)/(2*(r**2 + softening**2)**(5/2)) * mass,axis=2).T\n",
    "   \n",
    "    jerk= None\n",
    "    pot = None\n",
    "\n",
    "    return acc, jerk, pot\n",
    "\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def parallel_acceleration_direct_fast(pos,mass,N,softening):\n",
    "    jerk = None\n",
    "    pot = None\n",
    "\n",
    "    # acc[i,:] ax,ay,az of particle i \n",
    "    #acc  = np.zeros([N,3])\n",
    "    acc = np.zeros_like(pos)\n",
    "\n",
    "    for i in prange(N-1):\n",
    "        for j in range(i+1,N):\n",
    "            # Compute relative acceleration given\n",
    "            # position of particle i and j\n",
    "            mass_1 = mass[i]\n",
    "            mass_2 = mass[j]\n",
    "\n",
    "            # Compute acceleration of particle i due to particle j\n",
    "            position_1=pos[i,:]\n",
    "            position_2=pos[j,:]\n",
    "            \n",
    "            # Cartesian component of the i,j particles distance\n",
    "            dx = position_1[0] - position_2[0]\n",
    "            dy = position_1[1] - position_2[1]\n",
    "            dz = position_1[2] - position_2[2]\n",
    "            \n",
    "\n",
    "            # Distance module\n",
    "            r = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "\n",
    "            # Cartesian component of the i,j force\n",
    "            acceleration = np.zeros(3)\n",
    "            acceleration[0] = -mass_2 * (5*softening**2 + 2*r**2) * dx / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[1] = -mass_2 * (5*softening**2 + 2*r**2) * dy / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[2] = -mass_2 * (5*softening**2 + 2*r**2) * dz / (2*(r**2 + softening**2)**(5/2))\n",
    "\n",
    "            # Update array with accelerations\n",
    "            acc[i,:] += acceleration\n",
    "            acc[j,:] -= mass_1 * acceleration / mass_2 # because acc_2nbody already multiply by m[j]\n",
    "        \n",
    "    return (acc,jerk,pot)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <figure>\n",
    "        <img src=\"./attachments_report_cpu_optimization/plots/numba_timings.png\" alt=\"Comparison Image\">\n",
    "    </figure>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `direct_slow` is by far the slowest. Among the optimized functions, we see from fastest to slowest:\n",
    "\n",
    "* `direct_parallel`: two nested for loops, using `njit(parallel=True)`. This is by far the fastest, by almost 3 to 5 times with respect to the others. It beats numpy's broadcasting too,showing that bypassing altogether numpy's interpretation, a machine-compiled code that uses all available cores is the fastest implementation possible.\n",
    "\n",
    "* `vectorized_slow`: vectorized version, without using numba. This is still faster than the numba-optimized version. This suggests that Numpy is already self-optimized and adding other computations to it only makes it slower. Take-home message: let numpy do its best and leave it alone.\n",
    "\n",
    "* `direct_fast`: two nested for loops, using` njit(parallel=False)`. Numba speeds up greatly the native version, but without parallelization it is still slower than numpy's broadcasting, that uses 4 cores on its own.\n",
    "\n",
    "* `vectorized_fast`: vectorized version, using `njit(parallel=False)`. As discussed in the second point, this only adds an overhead and actually slows down the already-optimized numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing - single evolution, parallel computations\n",
    "Here we use the `multiprocessing` library to tell python how to parallelize the code. As you can see, a Pool is created with a number of workers equal to the number of cores. All workers share the same memory storing the particles informations, by declaring global variables. To not run into a race condition, the function is rewritten in order for the workers to manage only a specified subset of particles. This is done by using the `map_async` function, that explicitly feeds slices of particles to the function that computes the acceleration.  \n",
    "\n",
    "Finally, we show an extra analysis comparing the performance of the `multiprocessing.pool.Pool` vs `multiprocessing.pool.ThreadPool` methods. Theory states that the latter should be used for IO-bound tasks, whilst the former class for CPU-bound tasks, which is our case [[3]](https://superfastpython.com/threadpool-vs-pool-in-python/). However, first euristical trials suggested that ThreadPool could be faster here. After a more statistically robust analysis, we find that the two methods are equivalent for our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### VECTORIZED ####\n",
    "\"\"\"\n",
    "Here a single run is evolved. It is the computation of the acceleration that is parallelized.\n",
    "\n",
    "\"\"\"\n",
    "from fireworks.ic import ic_two_body as ic_two_body\n",
    "from fireworks.ic import ic_random_uniform as ic_random_uniform\n",
    "\n",
    "from fireworks.nbodylib import dynamics as dyn\n",
    "from fireworks.nbodylib import integrators as intg\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import os \n",
    "\n",
    "\n",
    "\n",
    "def acceleration_direct_vectorized(N_particles, pos, mass):\n",
    "   \n",
    "    dx = pos[:, 0].reshape(N_particles, 1) - pos[:, 0] #broadcasting of (N,) on (N,1) array, obtain distance along x in an (N,N) matrix\n",
    "    dy = pos[:, 1].reshape(N_particles, 1) - pos[:, 1] \n",
    "    dz = pos[:, 2].reshape(N_particles, 1) - pos[:, 2] \n",
    "      \n",
    "    r = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "    r[r==0]=1\n",
    "    \n",
    "    dpos = np.concatenate((dx, dy, dz)).reshape((3,N_particles,N_particles)) \n",
    "\n",
    "\n",
    "    acc = - (dpos/r**3 @ mass).T\n",
    "    jerk= None\n",
    "    pot = None\n",
    "\n",
    "    return acc, jerk, pot\n",
    "\n",
    "def parallel_acc(a,b):\n",
    "\n",
    "    # global particles doesn't work\n",
    "    global pos\n",
    "    global N_particles\n",
    "    global mass\n",
    "  \n",
    "    N_subset = abs(b-a)\n",
    "\n",
    "    # Select particles from a to b to parallelize computation\n",
    "    # Need to rewrite the function in order to compute quantities of subset of particles wrt all the others\n",
    "    dx = pos[a:b, 0,np.newaxis] - pos[:, 0] #broadcasting of (N,) on (N,1) array, obtain distance along x in an (N,N) matrix\n",
    "    dy = pos[a:b, 1,np.newaxis] - pos[:, 1] \n",
    "    dz = pos[a:b, 2,np.newaxis] - pos[:, 2] \n",
    "      \n",
    "    r = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "    r[r==0]=1\n",
    "    # New dpos shape is (3,N_subset,N_particles) since \n",
    "    # 3 is the number of dimensions, \n",
    "    # N_subset is the number of particles in the subset and\n",
    "    # N_particles is the number of total particles\n",
    "    # dpos is the distance vector between each particle in the subset and all the others\n",
    "\n",
    "    dpos = np.concatenate((dx, dy, dz)).reshape((3,N_subset,N_particles)) \n",
    "   \n",
    "    acc = - (dpos/r**3 @ mass).T\n",
    "    jerk= None\n",
    "    pot = None\n",
    "\n",
    "    return acc, jerk, pot\n",
    "\n",
    "\n",
    "def parallel_integrator(a,b):\n",
    "    \n",
    "    global vel\n",
    "    global pos \n",
    "    global tstep\n",
    "    # global acc \n",
    "    # acceleration is needed only to update vel and pos, so it is not needed as a global variable\n",
    " \n",
    "    acc, _ , _ = parallel_acc(a,b) \n",
    "\n",
    "    # Euler integration\n",
    "    vel[a:b] = vel[a:b] + acc * tstep  # Update vel\n",
    "    pos[a:b] = pos[a:b] + vel[a:b] * tstep  # Update pos\n",
    "\n",
    "    # no need to update a global acceleration\n",
    "\n",
    "    # Return the updated particles, the acceleration, jerk (can be None), and potential (can be None)\n",
    "    return pos[a:b], vel[a:b]\n",
    "\n",
    "\n",
    "def parallel_evo(N_particles,total_evo_time):\n",
    "    global pos\n",
    "    global vel\n",
    "\n",
    "    #### MULTIPROCESSING ####\n",
    "    # define the number of processes\n",
    "    N_CORES = multiprocessing.cpu_count() # in my case 8 cores\n",
    "    N_PROCESSES = min(N_CORES, N_particles)\n",
    "    # create a pool of processes\n",
    "    pool = Pool(N_PROCESSES) \n",
    "\n",
    "    positions = []\n",
    "    # submit multiple instances of the function full_evo \n",
    "    # - starmap_async: allows to run the processes with a (iterable) list of arguments\n",
    "    # - map_async    : is a similar function, supporting a single argument\n",
    "    for _ in range(int(total_evo_time/tstep)):\n",
    "        if N_particles < N_PROCESSES:\n",
    "            # 1 process per particle\n",
    "            future_results = pool.starmap_async(parallel_integrator, \n",
    "                                        [(i, (i + 1)) for i in range(N_particles)])\n",
    "        else:\n",
    "            # divide in equal part the particles into N_PROCESSES\n",
    "            future_results = pool.starmap_async(parallel_integrator, \n",
    "                                    [(i * N_particles // N_PROCESSES, (i + 1) * N_particles // N_PROCESSES) for i in range(N_PROCESSES)])\n",
    "\n",
    "\n",
    "\n",
    "        # to get the results all processes must have been completed\n",
    "        # the get() function is therefore _blocking_ (equivalent to join) \n",
    "        results = future_results.get()\n",
    "        \n",
    "        # update global variables directly\n",
    "        pos = np.concatenate([results[i][0] for i in range(len(results))])\n",
    "        vel = np.concatenate([results[i][1] for i in range(len(results))])\n",
    "        \n",
    "        positions.append(pos)\n",
    "    \n",
    "\n",
    "    # close the pool\n",
    "    # Warning multiprocessing.pool objects have internal resources that need to be properly managed \n",
    "    # (like any other resource) by using the pool as a context manager or by calling close() and terminate() manually. Failure to do this can lead to the process hanging on finalization.\n",
    "    pool.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(n_particles):\n",
    "    # Mock main function, just to show how to use the parallel_evo function\n",
    "    # For actual implementation, see the .py file (probably not submitted as attachement btw)\n",
    "    global pos\n",
    "    global vel\n",
    "    global mass\n",
    "    global N_particles\n",
    "    global tstep\n",
    "    \n",
    "    particles = ic_random_uniform(n_particles, [0,3],[0,3],[0,3])\n",
    "    pos = particles.pos\n",
    "    vel = particles.vel\n",
    "    mass = particles.mass\n",
    "    N_particles = len(particles)   \n",
    "    tstep = 0.01\n",
    "\n",
    "    # Run parallel simulations\n",
    "    results = parallel_evo(N_particles, total_evo_time=tstep)\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <figure>\n",
    "        <img src=\"./attachments_report_cpu_optimization/plots/comparison_single_evo_parallel_compute.png\" alt=\"Comparison Image\">\n",
    "    </figure>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our multiprocessing implementation succeedingly speeds up the code up to $\\sim 4$ times. Pay attention to the fact that for less than $\\sim 1000$ particles there is no benefit in introducing multiprocessing. The speed-up is due to the fact that multiprocessing uses all the cores available. We already mentioned that our standard vectorized function uses 4 cores, thus we expected multiprocessing to take half time, since it uses 8 cores on our machine. Instead, it accelerates even more, showing that there is a better memory management.\n",
    "\n",
    "The best optimization is reached by parallelizing the vectorized function. Contrary to the numba optimization, numpy benefits from multiprocessing in this case. This may be due to the fact that we do not modify the behaviour of numpy per se, but instead we just control how memory is distributed across cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss ThreadPool vs Pool. In the following plot we see that the two methods are completely equal, at least in the range of particles that we assessed. We may see a difference if the task had a bigger I/O bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <figure>\n",
    "        <img src=\"./attachments_report_cpu_optimization/plots/Pool_vs_ThreadPool.png\" alt=\"Comparison Image\">\n",
    "    </figure>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba vs Multiprocessing.\n",
    "What's the fastest method to compute acceleration?\n",
    "\n",
    "We find that the parallelized version of numba direct is 5 times faster than the multiprocessed vectorized function. Another sign that a compilied version of the code is better than an interpreted one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <figure>\n",
    "        <img src=\"./attachments_report_cpu_optimization/plots/numba_vs_pool.png\" alt=\"Comparison Image\">\n",
    "    </figure>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing - parallel evolutions, serial acceleration estimate\n",
    "\n",
    "We now use multiprocessing to run multiple simulations in parallel. Each process relates to an entire set of particles that is completely evolved; each process is independent from one another. Within each process, there is no optimization of the acceleration estimate (see next section for an implementation of both). \n",
    "\n",
    "We compare the results with the time it takes for Python to run the different simulations in a sequential native for loop. We find that our apporach speeds-up computations significantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evolve multiple integrators in parallel and compare the results with serial evolution.\n",
    "Here we use our fireworks implementation without modifications.\n",
    "\"\"\"\n",
    "\n",
    "from fireworks.ic import ic_two_body as ic_two_body\n",
    "from fireworks.ic import ic_random_uniform\n",
    "\n",
    "from fireworks.nbodylib import dynamics as dyn\n",
    "from fireworks.nbodylib import integrators as intg\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "\n",
    "def simulate(int_part,tstep=0.01,total_time = 10.0):\n",
    "   \n",
    "   integrator, particles = int_part\n",
    "   N_particles = len(particles)\n",
    "\n",
    "   integrator_name = integrator.__name__\n",
    "   acc_list = np.array([])\n",
    "   pos_list = np.array([])\n",
    "   vel_list = np.array([])\n",
    "   kinetic_list   = np.array([])\n",
    "   potential_list = np.array([])\n",
    "   energy_list    = np.array([])\n",
    "   \n",
    "   # Uncomment the following to run the simulation for multiple timesteps\n",
    "   #for _ in range(int(total_time/tstep)): \n",
    "    \n",
    "   particles, tstep, acc, jerk, _ = integrator(particles=particles, \n",
    "                                                tstep=tstep, \n",
    "                                                acceleration_estimator=dyn.acceleration_direct_vectorized,\n",
    "                                                softening=0.1,\n",
    "                                                )\n",
    "   \n",
    "   acc_list = np.append(acc_list, acc)\n",
    "   pos_list = np.append(pos_list, particles.pos)\n",
    "   vel_list = np.append(vel_list, particles.vel)\n",
    "\n",
    "   kinetic_list   = np.append(kinetic_list, particles.Ekin())\n",
    "   potential_list = np.append(potential_list, particles.Epot(softening=0.1))\n",
    "   energy_list    = np.append(energy_list, particles.Etot(softening=0.1))\n",
    "\n",
    "\n",
    "   acc_list = acc_list.reshape(int(total_time/tstep), N_particles, 3)\n",
    "   pos_list = pos_list.reshape(int(total_time/tstep), N_particles, 3)\n",
    "   vel_list = vel_list.reshape(int(total_time/tstep), N_particles, 3)\n",
    "   \n",
    "   return {\"integrator_name\": integrator_name,\"acc_list\": acc_list, \"pos_list\": pos_list, \"vel_list\": vel_list, \"energy_list\": energy_list}\n",
    "      \n",
    "\n",
    "\n",
    "def parallel_evo(integrators,particles):\n",
    "    \n",
    "    #### MULTIPROCESSING ####\n",
    "    \n",
    "    # create a pool of processes\n",
    "    pool = Pool() # default number of processes is os.cpu_count()\n",
    "\n",
    "\n",
    "    # submit multiple instances of the function full_evo \n",
    "    # - starmap_async: allows to run the processes with a (iterable) list of arguments\n",
    "    # - map_async    : is a similar function, supporting a single argument\n",
    "\n",
    "    future_results = pool.map_async(simulate, [(integrator,particles) for integrator in integrators])\n",
    "\n",
    "    # to get the results all processes must have been completed\n",
    "    # the get() function is therefore _blocking_ (equivalent to join) \n",
    "    results = future_results.get()\n",
    "  \n",
    "\n",
    "    # close the pool\n",
    "    # Warning multiprocessing.pool objects have internal resources that need to be properly managed \n",
    "    # (like any other resource) by using the pool as a context manager or by calling close() and terminate() manually. Failure to do this can lead to the process hanging on finalization.\n",
    "    pool.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main(n_particles=2, n_simulations=1):\n",
    "    \n",
    "    # To make this more readable, I delete the part where the results are save and time is computed\n",
    "   \n",
    "    # Create a set of particles\n",
    "    particles = ic_random_uniform(n_particles, [1,3],[1,3],[1,3])\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Here I will parallelize the same simulation with the same integrators.\n",
    "    If you want to parallelize with different integrators, use: \n",
    "\n",
    "    integrators = [intg.integrator_euler,\n",
    "                    intg.integrator_hermite,\n",
    "                    intg.integrator_leapfrog,\n",
    "                    intg.integrator_heun,\n",
    "                    intg.integrator_rk4,\n",
    "                    ]\n",
    "    \"\"\"\n",
    "    # Using only leapfrog integrator\n",
    "    integrators = [intg.integrator_leapfrog for _ in range(n_simulations)]\n",
    "\n",
    "    # MULTIPROCESSING \n",
    "    results = parallel_evo(integrators,particles)\n",
    "   \n",
    "    # Serial\n",
    "    results_serial = [simulate((integrator,particles)) for integrator in integrators]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <figure>\n",
    "        <img src=\"./attachments_report_cpu_optimization/plots/parallel_vs_serial.png\" alt=\"Comparison Image\">\n",
    "    </figure>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the parallel simulations outperform the serial ones, only if more than $\\sim 500$ particles per simulation are considered. Otherwise, there will be only an overhead introduced by the Pool.\n",
    "\n",
    "We show the first plot (Number of simulations = 1) as benchmark, and indeed in that case serial is better because there are multiple workers that try to split the task between them, resulting in an inefficient waste of resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Numba NJIT on parallel simulations\n",
    "\n",
    "Now, let's put everything together and see if we can run multiple simulations in parallel, each of them computing the accelerations using the optimized numba njit functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evolve multiple integrators in parallel and compare the results with serial evolution.\n",
    "\n",
    "Using the fastest acceleration function found with Numba (parallel_acceleration_direct_fast)\n",
    "and parallelizing the evolution of multiple integrators using the multiprocessing library.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import fireworks\n",
    "from fireworks.ic import ic_two_body as ic_two_body\n",
    "from fireworks.ic import ic_random_uniform\n",
    "\n",
    "from fireworks.nbodylib import dynamics as dyn\n",
    "from fireworks.nbodylib import integrators as intg\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os \n",
    "import time\n",
    "\n",
    "from numba import njit,prange\n",
    "\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def parallel_acceleration_direct_fast(in_list):\n",
    "    pos,N,mass,softening = in_list\n",
    "    jerk = None\n",
    "    pot = None\n",
    "\n",
    "    acc = np.zeros_like(pos)\n",
    "\n",
    "    for i in prange(N-1):\n",
    "        for j in range(i+1,N):\n",
    "            # Compute relative acceleration given\n",
    "            # position of particle i and j\n",
    "            mass_1 = mass[i]\n",
    "            mass_2 = mass[j]\n",
    "\n",
    "            # Compute acceleration of particle i due to particle j\n",
    "            position_1=pos[i,:]\n",
    "            position_2=pos[j,:]\n",
    "            \n",
    "            # Cartesian component of the i,j particles distance\n",
    "            dx = position_1[0] - position_2[0]\n",
    "            dy = position_1[1] - position_2[1]\n",
    "            dz = position_1[2] - position_2[2]\n",
    "            \n",
    "\n",
    "            # Distance module\n",
    "            r = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "\n",
    "            # Cartesian component of the i,j force\n",
    "            acceleration = np.zeros(3)\n",
    "            acceleration[0] = -mass_2 * (5*softening**2 + 2*r**2) * dx / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[1] = -mass_2 * (5*softening**2 + 2*r**2) * dy / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[2] = -mass_2 * (5*softening**2 + 2*r**2) * dz / (2*(r**2 + softening**2)**(5/2))\n",
    "\n",
    "            # Update array with accelerations\n",
    "            acc[i,:] += acceleration\n",
    "            acc[j,:] -= mass_1 * acceleration / mass_2 # because acc_2nbody already multiply by m[j]\n",
    "        \n",
    "    return (acc,jerk,pot)\n",
    "\n",
    "\n",
    "def parallel_evo(pos,mass,N,softening,n_simulations):\n",
    "    \n",
    "    #### MULTIPROCESSING ####\n",
    "    # define the number of processes\n",
    "    #N_CORES = multiprocessing.cpu_count() # in my case 4 cores\n",
    "    #N_INTEGRATORS = len(integrators)\n",
    "    # start a timer\n",
    "    #start = time.time()\n",
    "    \n",
    "    # create a pool of processes\n",
    "    pool = Pool()\n",
    "\n",
    "\n",
    "    # submit multiple instances of the function full_evo \n",
    "    # - starmap_async: allows to run the processes with a (iterable) list of arguments\n",
    "    # - map_async    : is a similar function, supporting a single argument\n",
    "   \n",
    "    future_results = pool.map_async(parallel_acceleration_direct_fast, [(pos,N,mass,softening) for _ in range(n_simulations)])\n",
    "\n",
    "    # to get the results all processes must have been completed\n",
    "    # the get() function is therefore _blocking_ (equivalent to join) \n",
    "    results = future_results.get()\n",
    "  \n",
    "\n",
    "    # close the pool\n",
    "    # Warning multiprocessing.pool objects have internal resources that need to be properly managed \n",
    "    # (like any other resource) by using the pool as a context manager or by calling close() and terminate() manually. Failure to do this can lead to the process hanging on finalization.\n",
    "    pool.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main(n_particles, n_simulations):\n",
    "    \n",
    "    particles = ic_random_uniform(n_particles, [1,3],[1,3],[1,3])\n",
    "\n",
    "    pos = particles.pos\n",
    "    mass = particles.mass\n",
    "    N = len(particles)\n",
    "    softening = 0.01\n",
    "\n",
    "    # compile function\n",
    "    _ = parallel_acceleration_direct_fast((pos,N,mass,softening))\n",
    "\n",
    "    # MULTIPROCESSING\n",
    "    _ = parallel_evo(pos,mass,N,softening,n_simulations)\n",
    "\n",
    "\n",
    "    # Serial\n",
    "    for i in range(n_simulations):\n",
    "        _ = parallel_acceleration_direct_fast((pos,N,mass,softening))\n",
    "    \n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <figure>\n",
    "        <img src=\"./attachments_report_cpu_optimization/plots/for_numba_parallel_mp_and_njit.png\" alt=\"Comparison Image\">\n",
    "    </figure>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using multiprocessing only adds a big overhead to NJIT, that confirms to be the fastest method we find to compute accelerations, both for one and for multiple simulations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
