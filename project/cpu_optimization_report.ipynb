{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cpu optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we assess different methods to optimize N-body simulations with Python on CPU.\n",
    "This task is approached in two ways: \n",
    "1.  Optimizing the computation of accelerations, for a given single simulation;\n",
    "\n",
    "2. Optimizing independent simulations run in parallel.\n",
    "\n",
    "For the first approach we use the built-in `multiprocessing` and the external `numba` library, while for the second only the former is considered. \n",
    "\n",
    "We find a significant speed-up of the code in both cases (**aggiungi quanto**), even though optimization fails when we try to mix the two methods, i.e. when running multiple independent simulations, while computing the accelerations in an optimized fashion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rimuovi il seguente poi**\n",
    "* what is Multiprocessing? (how cpu works, ALU)\n",
    "* what is the GIL? (what's njit and parallel?)\n",
    "\n",
    "* how did you implement the codes?\n",
    "* final considerations --> who wins?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using all cores in a single simulation\n",
    "Modern CPUs come with multiple cores that may run multiple computations, if well programmed. A naive approach to estimating the accelerations in a N-body simulation consists in writing two nested for loops, that for each particle compute the force with respect to all others. ** add code? ** We refer to this as a *direct* estimation of the forces. Since Python is an interpreted language, direct estimation performs very badly, and indeed we see that only one core is used when using this method (this is done by visually insepcting the cores used, running `htop` in a bash terminal). ** aggiungere immagine htop??** \n",
    "A first optimization is implemented by exploiting Numpy's broadcasting. We refer to this as a *vectorized* estimation. By doing so, we see that Numpy automatically uses 4 cores, instead of just one (Numpy is written in compiled C). This results in a much faster estimation,* as discussed in the assignments of this class ???* .  \n",
    "\n",
    "In a 8-core CPU this is still not the best result achievable, since there are still 4 unused cores. Here we explore the idead that, since Numpy uses 4 cores, we should be able to run our estimations at least two times faster, when using a 8-core CPU. The gain obtained will be greater as the number of cores available gets bigger.\n",
    "\n",
    "This is implemented by using both the `multiprocessing` and `numba` libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing\n",
    "\n",
    "Another possibility is to use all the cores available to run multiple independent simulations, allocating a subset of the CPU each. This goes under the name of **multiprocessing**, and each independent simulation is called a **process**.  It is important to remark that each process has an independent memory, thus it does not communicate with the other processes, even though they run on the same CPU at the same time. This feature discriminate it from **multithreading**, which shares a common memory between different threads running in parallel.\n",
    "\n",
    "Multiprocessing is also used to parallelize computations of the acceleration in a single run, by assigning a particles subset to each process.\n",
    "\n",
    "Each process is taken care of by a **worker**. The set of all workers is called **Pool**. It is good practice to create a Pool just once, but in our implementation it is sometimes opened and closed at each iteration of the evolution, as we will discuss later. In any case, this does not cause a too large overhead. \n",
    "\n",
    "* aggiungi immagine multiprocessing ? * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba's JIT compilation and parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, Python is an interpreted language meaning that a program directly executes instructions, without requiring them previously to have been compiled into a machine language program [[1]](https://en.wikipedia.org/wiki/Interpreter_(computing)). This causes native Python to be much slower than C++, for example. \n",
    "\n",
    "Fortunately, it is possible to speed up the code using `numba`, which compiles it to machine code “just-in-time” for execution, making it run at native machine code speed, when possible [[2]](https://numba.readthedocs.io/en/stable/user/5minguide.html). Numba works well with Numpy arrays and it is easy to implement in a standard Python script, since it requires to add just a decorator on our functions.\n",
    "\n",
    "Numba offers the possibilty to run the code in a multi-threaded fashion, by setting `parallel=True` inside the decorator. Unfortunately, not all codes can be parallelized in this way. In fact, we have been able to use this feature only on the direct estimation, and not on the vectorized one.\n",
    "\n",
    "We use numba to speed up the estimation of accelerations for a single simulation. We refer to this as *NJIT* estimation, which stands for \"no python - just in time\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and results\n",
    "\n",
    "All results shown in the plots are obtained on separate .py files.  \n",
    "Let's start by considering parallel accelerations computations on a single evolution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba\n",
    "\n",
    "Here we show the direct and the vectorized functions used, with their numba's counterparts. Notice that numba requires just to add a decorator `@njit` at the beginning of the function, and to use `prange` instead of `range`.\n",
    "\n",
    "In addition, numpy may need some chenges in order for numba to understand exactly how some objects look like when compiled. This fact is a clear example of the work of Python's interpreter that happens \"under the hood\". For example \n",
    "\n",
    "```python\n",
    "acc  = np.zeros([N,3]) # This does not work with numba\n",
    "acc  = np.zeros_like(pos) # Use this instead\n",
    "```\n",
    "Or when using reshape, it is necessary to make a copy of the array, in order to make it C-contiguous in the memory.\n",
    "\n",
    "```python\n",
    "dx = pos[:, 0].reshape(N_particles, 1) - pos[:, 0] # This does not work with numba\n",
    "dx = pos[:, 0].copy().reshape(N_particles, 1) - pos[:, 0]  # Use this instead\n",
    "   \n",
    "```\n",
    "\n",
    "\n",
    "The biggest problems arise when trying to compute a matrix multiplication using `parallel=True`. Indeed, `np.matmul` is not implemented and even trying working around this, we are not able to use our vectorized function in a parallel way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fireworks.ic import ic_random_uniform as ic_random_uniform\n",
    "import numpy as np\n",
    "from numba import prange, njit\n",
    "\n",
    "def acceleration_direct_slow(pos,mass,N,softening):\n",
    "    jerk = None\n",
    "    pot = None\n",
    "\n",
    "    # acc[i,:] ax,ay,az of particle i \n",
    "    acc  = np.zeros([N,3])\n",
    "\n",
    "    for i in range(N-1):\n",
    "        for j in range(i+1,N):\n",
    "            # Compute relative acceleration given\n",
    "            # position of particle i and j\n",
    "            mass_1 = mass[i]\n",
    "            mass_2 = mass[j]\n",
    "\n",
    "            # Compute acceleration of particle i due to particle j\n",
    "            position_1=pos[i,:]\n",
    "            position_2=pos[j,:]\n",
    "            \n",
    "            # Cartesian component of the i,j particles distance\n",
    "            dx = position_1[0] - position_2[0]\n",
    "            dy = position_1[1] - position_2[1]\n",
    "            dz = position_1[2] - position_2[2]\n",
    "            \n",
    "\n",
    "            # Distance module\n",
    "            r = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "\n",
    "            # Cartesian component of the i,j force\n",
    "            acceleration = np.zeros(3)\n",
    "            acceleration[0] = -mass_2 * (5*softening**2 + 2*r**2) * dx / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[1] = -mass_2 * (5*softening**2 + 2*r**2) * dy / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[2] = -mass_2 * (5*softening**2 + 2*r**2) * dz / (2*(r**2 + softening**2)**(5/2))\n",
    "\n",
    "            # Update array with accelerations\n",
    "            acc[i,:] += acceleration\n",
    "            acc[j,:] -= mass_1 * acceleration / mass_2 # because acc_2nbody already multiply by m[j]\n",
    "        \n",
    "    return (acc,jerk,pot)\n",
    "\n",
    "\n",
    "@njit\n",
    "def acceleration_direct_fast(pos,mass,N,softening):\n",
    "    jerk = None\n",
    "    pot = None\n",
    "\n",
    "    # acc[i,:] ax,ay,az of particle i \n",
    "    #acc  = np.zeros([N,3])\n",
    "    acc = np.zeros_like(pos)\n",
    "\n",
    "    for i in range(N-1):\n",
    "        for j in range(i+1,N):\n",
    "            # Compute relative acceleration given\n",
    "            # position of particle i and j\n",
    "            mass_1 = mass[i]\n",
    "            mass_2 = mass[j]\n",
    "\n",
    "            # Compute acceleration of particle i due to particle j\n",
    "            position_1=pos[i,:]\n",
    "            position_2=pos[j,:]\n",
    "            \n",
    "            # Cartesian component of the i,j particles distance\n",
    "            dx = position_1[0] - position_2[0]\n",
    "            dy = position_1[1] - position_2[1]\n",
    "            dz = position_1[2] - position_2[2]\n",
    "            \n",
    "\n",
    "            # Distance module\n",
    "            r = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "\n",
    "            # Cartesian component of the i,j force\n",
    "            acceleration = np.zeros(3)\n",
    "            acceleration[0] = -mass_2 * (5*softening**2 + 2*r**2) * dx / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[1] = -mass_2 * (5*softening**2 + 2*r**2) * dy / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[2] = -mass_2 * (5*softening**2 + 2*r**2) * dz / (2*(r**2 + softening**2)**(5/2))\n",
    "\n",
    "            # Update array with accelerations\n",
    "            acc[i,:] += acceleration\n",
    "            acc[j,:] -= mass_1 * acceleration / mass_2 # because acc_2nbody already multiply by m[j]\n",
    "        \n",
    "    return (acc,jerk,pot)\n",
    "\n",
    "def slow_acceleration_direct_vectorized(pos,N_particles,mass,softening):\n",
    "    dx = pos[:, 0].reshape(N_particles, 1) - pos[:, 0] #broadcasting of (N,) on (N,1) array, obtain distance along x in an (N,N) matrix\n",
    "    dy = pos[:, 1].reshape(N_particles, 1) - pos[:, 1] \n",
    "    dz = pos[:, 2].reshape(N_particles, 1) - pos[:, 2] \n",
    "    \n",
    "    r = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "    r[r==0]=1\n",
    "    \n",
    "    dpos = np.concatenate((dx, dy, dz)).reshape((3,N_particles,N_particles)) \n",
    "    acc = - (dpos* (5*softening**2 + 2*r**2)/(2*(r**2 + softening**2)**(5/2)) @ mass).T\n",
    "    \n",
    "    jerk= None \n",
    "    pot = None\n",
    "\n",
    "    return acc, jerk, pot\n",
    "\n",
    "\n",
    "@njit\n",
    "def fast_acceleration_direct_vectorized(pos,N_particles,mass,softening):\n",
    "   \n",
    "    dx = pos[:, 0].copy().reshape(N_particles, 1) - pos[:, 0] #broadcasting of (N,) on (N,1) array, obtain distance along x in an (N,N) matrix\n",
    "    dy = pos[:, 1].copy().reshape(N_particles, 1) - pos[:, 1] \n",
    "    dz = pos[:, 2].copy().reshape(N_particles, 1) - pos[:, 2] \n",
    "      \n",
    "    r = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "    #r[r==0]=1 not supported on numba\n",
    "    r += np.eye(r.shape[0])\n",
    "    \n",
    "    dpos = np.concatenate((dx, dy, dz)).copy().reshape((3,N_particles,N_particles)) \n",
    "    acc = - np.sum(dpos* (5*softening**2 + 2*r**2)/(2*(r**2 + softening**2)**(5/2)) * mass,axis=2).T\n",
    "   \n",
    "    jerk= None\n",
    "    pot = None\n",
    "\n",
    "    return acc, jerk, pot\n",
    "\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def parallel_acceleration_direct_fast(pos,mass,N,softening):\n",
    "    jerk = None\n",
    "    pot = None\n",
    "\n",
    "    # acc[i,:] ax,ay,az of particle i \n",
    "    #acc  = np.zeros([N,3])\n",
    "    acc = np.zeros_like(pos)\n",
    "\n",
    "    for i in prange(N-1):\n",
    "        for j in range(i+1,N):\n",
    "            # Compute relative acceleration given\n",
    "            # position of particle i and j\n",
    "            mass_1 = mass[i]\n",
    "            mass_2 = mass[j]\n",
    "\n",
    "            # Compute acceleration of particle i due to particle j\n",
    "            position_1=pos[i,:]\n",
    "            position_2=pos[j,:]\n",
    "            \n",
    "            # Cartesian component of the i,j particles distance\n",
    "            dx = position_1[0] - position_2[0]\n",
    "            dy = position_1[1] - position_2[1]\n",
    "            dz = position_1[2] - position_2[2]\n",
    "            \n",
    "\n",
    "            # Distance module\n",
    "            r = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "\n",
    "            # Cartesian component of the i,j force\n",
    "            acceleration = np.zeros(3)\n",
    "            acceleration[0] = -mass_2 * (5*softening**2 + 2*r**2) * dx / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[1] = -mass_2 * (5*softening**2 + 2*r**2) * dy / (2*(r**2 + softening**2)**(5/2))\n",
    "            acceleration[2] = -mass_2 * (5*softening**2 + 2*r**2) * dz / (2*(r**2 + softening**2)**(5/2))\n",
    "\n",
    "            # Update array with accelerations\n",
    "            acc[i,:] += acceleration\n",
    "            acc[j,:] -= mass_1 * acceleration / mass_2 # because acc_2nbody already multiply by m[j]\n",
    "        \n",
    "    return (acc,jerk,pot)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
